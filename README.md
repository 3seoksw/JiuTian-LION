# JiuTian
<div align="center">

<h1>JiuTian (九天) </h1>

<div>
:fire: Details will be released. Stay tuned :beers: :+1: 
</div>
<br>
  
<img src='assets/model.jpg' width='100%'>

</div>

## Training Data

|Dataset|Size|Pretraining|Instruction Tuning|
|:-|:-:|:-:|:-:|
|[LAION](https://laion.ai/)(*)|100M| :white_check_mark:
|[COCOCN](https://github.com/li-xirong/coco-cn)|27K|:white_check_mark:|
|[MIMIC-IT](https://github.com/Luodian/Otter/blob/main/mimic-it/README.md)(**)|430K||:white_check_mark:
|[LRV](https://fuxiaoliu.github.io/LRV/)|20K||:white_check_mark:
|[LLaVAR](https://llavar.github.io/)|158K||:white_check_mark:
|[TextCap](https://textvqa.org/textcaps/)|109K||:white_check_mark:
|[VQA v2.0](https://visualqa.org/)|443K||:white_check_mark:
|[GQA](https://github.com/stanfordnlp/mac-network)|943K||:white_check_mark:
|[IconQA](https://iconqa.github.io/)|19K||:white_check_mark:
|[OK-VQA](https://okvqa.allenai.org/)|9K||:white_check_mark:
|[A-OKVQA](https://allenai.org/project/a-okvqa/home)|17K||:white_check_mark:
|[TextVQA](https://textvqa.org/)|35K||:white_check_mark:|
|[Visual Dialog](https://visualdialog.org/)|123K||:white_check_mark:
|[COCO](https://cocodataset.org/#home)|414K||:white_check_mark:
|[VSR](https://github.com/cambridgeltl/visual-spatial-reasoning)|11K||:white_check_mark:

Notes:
(*): using our designed rules to filter the original data;
(**): only including the [LA](https://entuedu-my.sharepoint.com/personal/libo0013_e_ntu_edu_sg/_layouts/15/onedrive.aspx?ga=1&id=%2Fpersonal%2Flibo0013%5Fe%5Fntu%5Fedu%5Fsg%2FDocuments%2FMIMIC%2DIT%2DRelease%2FLA%2Ezip&parent=%2Fpersonal%2Flibo0013%5Fe%5Fntu%5Fedu%5Fsg%2FDocuments%2FMIMIC%2DIT%2DRelease) part from its open source data
